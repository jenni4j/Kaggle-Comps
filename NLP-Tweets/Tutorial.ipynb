{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlppreprocess in /Users/Jenny/anaconda3/lib/python3.7/site-packages (1.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "    \n",
    "!pip install nlppreprocess\n",
    "from nlppreprocess import NLP\n",
    "\n",
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets\n",
    "train_df = pd.read_csv('input/train.csv')\n",
    "test_df = pd.read_csv('input/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's up man?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a non-disaster tweet\n",
    "train_df[train_df[\"target\"] == 0][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a disaster tweet\n",
    "train_df[train_df[\"target\"] == 1][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Missing Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>33.272035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>0.801261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Missing Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>33.864542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>0.796813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at missing values of each variable\n",
    "train_perc_missing = train_df.isnull().mean()*100\n",
    "test_perc_missing = test_df.isnull().mean()*100\n",
    "\n",
    "df1 = pd.DataFrame({'Train Missing Percentage': train_perc_missing.sort_values(ascending=False)})\n",
    "df2 = pd.DataFrame({'Test Missing Percentage': test_perc_missing.sort_values(ascending=False)})\n",
    "display_side_by_side(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e740e10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at distribution of disaster / non-disaster tweets\n",
    "sns.barplot(train_df['target'].value_counts().index, train_df['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing - lowercase everything\n",
    "def lowercase_text(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x: lowercase_text(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: lowercase_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing - removing text noise\n",
    "\n",
    "def remove_noise(text):\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x: remove_noise(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing - remove stopwords\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(nlp.process)\n",
    "test_df['text'] = test_df['text'].apply(nlp.process)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing - stemming (removing everything but root word)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing - others\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x : remove_html(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x : remove_html(x))\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x : remove_emoji(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x : remove_emoji(x))\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x : remove_punct(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x : remove_punct(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "# we can look at the first 5 tweets in the data and get counts for them\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 52)\n",
      "[[0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# there are 54 unique words (or tokens) in the first 5 tweets\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "# the first tweet contains only some of those unique tokens\n",
    "print(example_train_vectors[0].todense())\n",
    "# remaining questions, what are the rest of the components of example_train_vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create vectors for all of our tweets\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "#remaining questions, why use .transform vs .fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TFIDF\n",
    "tfidf = feature_extraction.text.TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "tfidf.fit(train_df['text'])\n",
    "train_tfidf = tfidf.fit_transform(train_df['text'])\n",
    "test_tfidf = tfidf.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62201454 0.55165692 0.63259402]\n",
      "[0.62057522 0.56656347 0.62712739]\n"
     ]
    }
   ],
   "source": [
    "# use ridge regression in order to push model weights to 0 without\n",
    "# completely discounting different words -- look further into this\n",
    "clf_RR = RidgeClassifier()\n",
    "\n",
    "# let's test our model to see how well it does on training data\n",
    "# we can do this by cross validation (train portion of training data & validate with rest)\n",
    "\n",
    "# metric for this competition is F1\n",
    "# look further into the function cross_val_score & its outputs\n",
    "\n",
    "cv_scores = model_selection.cross_val_score(clf_RR, train_vectors, train_df[\"target\"], cv=3, scoring='f1')\n",
    "print(cv_scores)\n",
    "\n",
    "tf_scores = model_selection.cross_val_score(clf_RR, train_tfidf, train_df[\"target\"], cv=3, scoring='f1')\n",
    "print(tf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jenny/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62184874 0.54421769 0.60411523 0.58034894 0.71742543]\n",
      "[0.6035503  0.53507951 0.58076225 0.56868396 0.70389171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jenny/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# try using logistic regression instead\n",
    "clf_LogR = LogisticRegression(C=1.0)\n",
    "cv_scores = model_selection.cross_val_score(clf_LogR, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(cv_scores)\n",
    "\n",
    "tf_scores = model_selection.cross_val_score(clf_LogR, train_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(tf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65937763 0.6250928  0.68995633 0.64686998 0.73991655]\n",
      "[0.59528487 0.56424079 0.62842012 0.59606373 0.74679487]\n"
     ]
    }
   ],
   "source": [
    "# try using Naive Bayes\n",
    "clf_NB = MultinomialNB()\n",
    "cv_scores = model_selection.cross_val_score(clf_NB, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(cv_scores)\n",
    "\n",
    "tf_scores = model_selection.cross_val_score(clf_NB, train_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(tf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jenny/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to do predictions on training set and build submission\n",
    "# clf_RR.fit(train_vectors, train_df[\"target\"])\n",
    "clf_LogR.fit(train_tfidf, train_df[\"target\"])\n",
    "#clf_NB.fit(train_tfidf, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = clf_LogR.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
